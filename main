#! /usr/bin/env python
# -*- coding: utf-8 -*-

# Copyright 2020 Imperial College London (Pingchuan Ma)
# Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)

import os
import random
import argparse
import numpy as np
import torch
import yaml
from datetime import datetime
from glob import glob
from torch.utils.tensorboard import SummaryWriter
from natsort import os_sorted
from data import DataScheduler
from models import MODEL
from train import train_model
from test import test_model
from utils.util import save2npz, calculateNorm2
from utils.dataloaders import get_preprocessing_pipelines
from utils.optim_utils import CosineScheduler

parser = argparse.ArgumentParser(description='Pytorch Lipreading')
# -- dataset config
parser.add_argument(
    '--config', '-c',
    default='configs/lrw_resnet18_mstcn.yaml'
)
parser.add_argument(
    '--log-dir', '-l',
    default='./log/' + datetime.now().strftime('%m-%d-%H:%M:%S')
)
parser.add_argument('--resume-ckpt')
parser.add_argument('--resume-latest-ckpt')
parser.add_argument('--override', default='')
parser.add_argument('--evaluate', '-e', default=False, action='store_true')
parser.add_argument('--test', default=False, action='store_true', help='training mode')
parser.add_argument('--extract-feats', default=False, action='store_true', help='Feature extractor')
parser.add_argument('--mouth-patch-path', type=str, default=None, help='Path to the mouth ROIs, assuming the file is saved as numpy.array')
parser.add_argument('--mouth-embedding-out-path', type=str, default=None, help='Save mouth embeddings to a specificed path')

def extract_feats(model):
    # not used
    """
    :rtype: FloatTensor
    """
    model.eval()
    preprocessing_func = get_preprocessing_pipelines()['test']
    data = preprocessing_func(np.load(args.mouth_patch_path)['data'])  # data: TxHxW
    return model(torch.FloatTensor(data)[None, None, :, :, :].cuda(), lengths=[data.shape[0]])

def main():
    args = parser.parse_args()
    config_path = args.config

    if args.resume_latest_ckpt:
        # resume from latest checkpoint
        log_dir = args.resume_latest_ckpt
        assert os.path.isdir(log_dir), 'log_dir {} does not exist'.format(log_dir)
        latest_path = os_sorted(glob(os.path.join(log_dir, 'ckpts/*')))[-1]
        print('Loading latest checkpoint: {}'.format(latest_path))
        base_dir = os.path.dirname(os.path.dirname(latest_path))
        config_path = os.path.join(base_dir, 'config.yaml')
        args.resume_ckpt = latest_path
    elif args.resume_ckpt:
        # resume from specified checkpoint
        base_dir = os.path.dirname(os.path.dirname(args.resume_ckpt))
        config_path = os.path.join(base_dir, 'config.yaml')
    config = yaml.load(open(config_path), Loader=yaml.FullLoader)

    # Override options
    for option in args.override.split('|'):
        if not option:
            continue
        address, value = option.split('=')
        keys = address.split('.')
        here = config
        for key in keys[:-1]:
            if key not in here:
                raise ValueError('{} is not defined in config file. '
                                 'Failed to override.'.format(address))
            here = here[key]
        if keys[-1] not in here:
            raise ValueError('{} is not defined in config file. '
                             'Failed to override.'.format(address))
        here[keys[-1]] = yaml.load(value, Loader=yaml.FullLoader)

    # Set log directory
    config['log_dir'] = args.log_dir
    if not args.resume_ckpt and os.path.exists(args.log_dir):
        print('WARNING: %s already exists' % args.log_dir)
        input('Press enter to continue')
        print()

    if args.resume_ckpt and not args.log_dir:
        config['log_dir'] = os.path.dirname(
            os.path.dirname(args.resume_ckpt)
        )

    # Save config
    os.makedirs(config['log_dir'], mode=0o755, exist_ok=True)
    if (not args.resume_ckpt or args.config) and (not args.test):
        config_save_path = os.path.join(config['log_dir'], 'config.yaml')
        yaml.dump(config, open(config_save_path, 'w'))
        print('Config saved to {}'.format(config['log_dir']))

    # set random seed
    if config['seed'] is not None:
        torch.manual_seed(config['seed'])
        np.random.seed(config['seed'])
        random.seed(config['seed'])
        torch.backends.cudnn.benchmark = True
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(config['seed'])

    # Build components
    writer = SummaryWriter(config['log_dir'])
    data_scheduler = DataScheduler(config)
    model = MODEL[config['model']](config, writer)
    calculateNorm2(model)
    model.to(config['device'])

    if args.resume_ckpt:
        # load trained model checkpoint
        checkpoint = torch.load(args.resume_ckpt)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        data_scheduler.epoch_cnt = checkpoint['epoch']
        if checkpoint.get('lr_scheduler_state_dict') is not None:
            model.lr_scheduler = CosineScheduler(
                    checkpoint['lr_scheduler_state_dict']['lr_ori'], checkpoint['lr_scheduler_state_dict']['epochs']
                )
            model.lr_scheduler.adjust_lr(model.optimizer, checkpoint['epoch'])
        resume_step = checkpoint['step'] + 1
        if (args.test is not None) and config.get('cache_only'):
            resume_step -= 1
    else:
        resume_step = 0

    # feature extraction (not used)
    if args.mouth_patch_path:
        save2npz(args.mouth_embedding_out_path, data=extract_feats(model).cpu().detach().numpy())
        return

    if args.test:
        test_model(config, model, data_scheduler, writer, resume_step)
    else:
        train_model(config, model, data_scheduler, writer, resume_step)

if __name__ == '__main__':
    main()
